{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "############## TENSORBOARD ########################\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/mnist1')\n",
    "###################################################\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/tensorboard.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(simple_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "model = simple_CNN(in_channels=in_channels, num_classes=num_classes).to(device)\n",
    "\n",
    "#Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "import torch\n",
    "writer = SummaryWriter('runs/example1')\n",
    "meta = []\n",
    "while len(meta)<100:\n",
    "    meta = meta+keyword.kwlist # get some strings\n",
    "meta = meta[:100]\n",
    "\n",
    "for i, v in enumerate(meta):\n",
    "    meta[i] = v+str(i)\n",
    "\n",
    "label_img = torch.rand(100, 3, 32, 32)\n",
    "for i in range(100):\n",
    "    label_img[i]*=i/100.0\n",
    "\n",
    "writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\n",
    "writer.add_embedding(torch.randn(100, 5), label_img=label_img)\n",
    "writer.add_embedding(torch.randn(100, 5), metadata=meta)\n",
    "writer.close()\n",
    "\n",
    "#https://tensorboardx.readthedocs.io/en/latest/tensorboard.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregando imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14b408f4890>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = iter(test_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "#print(example_data)\n",
    "plt.imshow(example_data[55][0])\n",
    "\n",
    "#for i in range(6):\n",
    " #  plt.subplot(2,3,i+1)\n",
    "  #  plt.imshow(example_data[i][0], cmap='gray')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "img_grid = torchvision.utils.make_grid(example_data)\n",
    "writer.add_image('mnist_images', img_grid)\n",
    "#writer.add_image('iwi',example_data[56])\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000014B2E651620>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\notebook.py\", line 283, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m      8\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m scores \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     10\u001b[0m loss \u001b[39m=\u001b[39m criterion(scores, targets)\n\u001b[0;32m     11\u001b[0m \u001b[39m# backward\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[98], line 11\u001b[0m, in \u001b[0;36msimple_CNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[1;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool(x)\n\u001b[0;32m     12\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n\u001b[0;32m     13\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[0;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step=0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Calculate running training accuracy\n",
    "        _, predictions = scores.max(1) # [[0,1],[1,0],[0,1],[]]\n",
    "               \n",
    "        num_correct = (predictions == targets).sum() #TP+TN\n",
    "        running_train_acc = float(num_correct)/float(data.shape[0]) #ACC\n",
    "\n",
    "        # tensorboard\n",
    "        writer.add_scalar('Training loss2', loss, global_step = step)\n",
    "        writer.add_scalar('Training Accurcy2', running_train_acc, global_step = step)\n",
    "        \n",
    "        step += 1 #step is the number of batches\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)`  \n",
    "\n",
    "Cuando ejecutas el método add_scalars varias veces con el mismo main_tag en diferentes ejecuciones, las gráficas se combinarán en TensorBoard, ya que TensorBoard mostrará todos los escalares con la misma etiqueta principal en una misma gráfica. Para corregir cambiaremos el tag o iniciar otro `writer = SummaryWriter('runs/mnist2') ` donde runs/mnist2 puede ser otra ruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mruns/MNIST/batchSize_\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m_lr_\u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)    \n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     34\u001b[0m         \u001b[39m# Get data to cuda if possible\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     36\u001b[0m         targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\Fernando Celis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], F_pil\u001b[39m.\u001b[39;49mget_image_num_channels(pic))\n\u001b[0;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "num_epochs = 3\n",
    "batch_sizes = [64,128,1024]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "\n",
    "# Load Data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),  download=True)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        step = 0        \n",
    "        \n",
    "        #dataloader\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        #Initialize network\n",
    "        model = simple_CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "        model.to(device) #move model to device\n",
    "\n",
    "        \n",
    "        #Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        #TENSORBOARD WRITER \n",
    "        writer = SummaryWriter(f'runs/MNIST/batchSize_{batch_size}_lr_{lr}')    \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                # Get data to cuda if possible\n",
    "                data = data.to(device=device)\n",
    "                targets = targets.to(device=device)\n",
    "\n",
    "                # forward\n",
    "                scores = model(data)\n",
    "                loss = criterion(scores, targets)\n",
    "\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient descent or adam step\n",
    "                optimizer.step()\n",
    "                \n",
    "                #Calculate running trining accuracy\n",
    "                _, predictions = scores.max(1)\n",
    "                num_correct = (predictions == targets).sum()\n",
    "                running_train_acc = float(num_correct)/float(data.shape[0])\n",
    "\n",
    "                # tensorboard\n",
    "                writer.add_scalar('Training loss', loss, global_step = step)\n",
    "                writer.add_scalar('Training Accurcy', running_train_acc, global_step = step)\n",
    "                step += 1 #step is the number of batches\n",
    "                \n",
    "        writer.add_hparams(\n",
    "            hparam_dict = {\"lr\": lr, \"bsize\": batch_size},\n",
    "            metric_dict = {\"Accuracy\": sum(accuracies)/len(accuracies), \"loss\": sum(losses)}\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` writer.add_hparams` agregar hyperparametros\n",
    "\n",
    "`writer.add_histogram ` Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m writer\u001b[39m.\u001b[39madd_histogram(\u001b[39m\"\u001b[39m\u001b[39mconv2\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m.\u001b[39mconv2\u001b[39m.\u001b[39mweight, global_step \u001b[39m=\u001b[39m step)\n\u001b[0;32m     58\u001b[0m \u001b[39m#Calculate running trining accuracy\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m _, predictions \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39;49mmax(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     60\u001b[0m num_correct \u001b[39m=\u001b[39m (predictions \u001b[39m==\u001b[39m targets)\u001b[39m.\u001b[39msum()\n\u001b[0;32m     61\u001b[0m running_train_acc \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(num_correct)\u001b[39m/\u001b[39m\u001b[39mfloat\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "batch_sizes = [128,1024]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "\n",
    "# Load Data\n",
    "#train_dataset = datasets.MNIST(root='dataset/', train=True, transform = transforms.ToTensor(), download=True)\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),  download=True)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        step = 0        \n",
    "        \n",
    "        #dataloader\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        #Initialize network\n",
    "        model = simple_CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "        model.to(device) #move model to device\n",
    "        \n",
    "        #Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        #TENSORBOARD WRITER \n",
    "        writer = SummaryWriter(f'runs/MNIST/batchSize_{batch_size}_lr_{lr}')    \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            accuracies, losses = [], []\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                # Get data to cuda if possible\n",
    "                data = data.to(device=device)\n",
    "                targets = targets.to(device=device)\n",
    "                \n",
    "                img_grid = torchvision.utils.make_grid(data) #make grid of images\n",
    "                writer.add_image(\"mnist_images\", img_grid)\n",
    "                # forward\n",
    "                scores = model(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient descent or adam step\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                # histogram of weights\n",
    "                writer.add_histogram(\"fc1\", model.fc1.weight, global_step = step)\n",
    "                writer.add_histogram(\"conv1\", model.conv1.weight, global_step = step)\n",
    "                writer.add_histogram(\"conv2\", model.conv2.weight, global_step = step)\n",
    "                \n",
    "                #Calculate running trining accuracy\n",
    "                _, predictions = scores.max(1)\n",
    "                num_correct = (predictions == targets).sum()\n",
    "                running_train_acc = float(num_correct)/float(data.shape[0])\n",
    "                accuracies.append(running_train_acc)\n",
    "                # tensorboard\n",
    "                writer.add_scalar('Training loss', loss, global_step = step)\n",
    "                writer.add_scalar('Training Accurcy', running_train_acc, global_step = step)\n",
    "                step += 1 #step is the number of batches\n",
    "\n",
    "        writer.add_hparams(\n",
    "            hparam_dict = {\"lr\": lr, \"bsize\": batch_size},\n",
    "            metric_dict = {\"Accuracy\": sum(accuracies)/len(accuracies), \"loss\": sum(losses)}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/469], Loss: 0.3626\n",
      "Epoch [1/1], Step [200/469], Loss: 0.3480\n",
      "Epoch [1/1], Step [300/469], Loss: 0.2180\n",
      "Epoch [1/1], Step [400/469], Loss: 0.1185\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "############## TENSORBOARD ########################\n",
    "writer.add_graph(model, example_data.reshape(-1, 28*28).to(device))\n",
    "###################################################\n",
    "\n",
    "# Train the model\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            ############## TENSORBOARD ########################\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
    "            running_accuracy = running_correct / 100 / predicted.size(0)\n",
    "            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
    "            running_correct = 0\n",
    "            running_loss = 0.0\n",
    "            ###################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
